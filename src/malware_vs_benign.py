
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd

from evaluate import evaluate_model

from sklearn.discriminant_analysis import StandardScaler
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report 
from sklearn.svm import SVC
                      
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier


df = pd.read_csv("data/malware_vs_benign.csv")

def preprocess(df):

    #create copy to keep pipeline happy
    meta = df[['Name']].copy()



    #delete dead weight columns
    df = df.drop(columns=['Name'])
    df = df.drop(columns=['e_magic'])

    #map categorical columns
    categorical_cols = ['Magic', 'Machine']
    
    for col in categorical_cols:
        #astype is a panda method to categorize
        df[col] = df[col].astype('category').cat.codes 

    # ---- Split Features vs Target
    X = df.drop(columns=['Malware'])
    y = df['Malware']

    return X, y, meta

X, y, meta = preprocess(df)

#----- Split -----
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, random_state=42, stratify=y
)


#### ----- Random Forest ------

rf = RandomForestClassifier(max_depth= 30, 
                            min_samples_leaf= 1, 
                            min_samples_split= 3, 
                            n_estimators= 252,
                            random_state=42)

#----- Train -----
rf.fit(X_train, y_train)

# ----- Evaluate -----
y_pred = rf.predict(X_test)

print("\n--- Random Forest ---")
print(classification_report(y_test, y_pred, digits=3))
print(confusion_matrix(y_test, y_pred))


#### ----- RGBoost ----



xgb = XGBClassifier(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train),  # balance weight
    random_state=42,
    eval_metric='logloss'  # suppresses warning
)
# Train 
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)



#------ SVM -------


#Only need to scale data for SVM model 
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

svm_model = SVC(kernel='rbf', class_weight='balanced', random_state=42)
svm_model.fit(X_train_scaled, y_train)



#### ------ Evaluate ----- 
# Random Forest
evaluate_model(rf, X_test, y_test, model_name="Random Forest")

# XGBoost
evaluate_model(xgb, X_test, y_test, model_name="XGBoost")

# SVM 
evaluate_model(svm_model, X_test, y_test, model_name="SVM")