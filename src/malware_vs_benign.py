
import warnings
warnings.filterwarnings("ignore")
import pandas as pd

from evaluate import evaluate_model

from sklearn.discriminant_analysis import StandardScaler
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.pipeline import Pipeline

                      
from xgboost import XGBClassifier


df = pd.read_csv("data/malware_vs_benign.csv")

def preprocess(df):

    #create copy to keep pipeline happy
    meta = df[['Name']].copy()



    #delete dead weight columns
    df = df.drop(columns=['Name'])
    df = df.drop(columns=['e_magic'])

    #map categorical columns
    categorical_cols = ['Magic', 'Machine']
    
    for col in categorical_cols:
        #astype is a panda method to categorize
        df[col] = df[col].astype('category').cat.codes 

    # ---- Split Features vs Target
    X = df.drop(columns=['Malware'])
    y = df['Malware']

    return X, y, meta

X, y, meta = preprocess(df)

#----- Split -----
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, random_state=42, stratify=y
)


#### ----- Random Forest ------

rf = RandomForestClassifier(max_depth= 30, 
                            min_samples_leaf= 1, 
                            min_samples_split= 3, 
                            n_estimators= 252,
                            random_state=42)

#----- Train -----
rf.fit(X_train, y_train)


#### ----- RGBoost ----

#Best parameters found using gridSearch (see ipynb)

xgb = XGBClassifier(
    n_estimators=200,
    learning_rate=0.1,
    scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train),  # balance weight
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric='logloss'  # suppresses warning
)

# Train 
xgb.fit(X_train, y_train)



#------ SVM -------


#Only need to scale data for SVM model 
# Final params based on ipynb search 
final_svm = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", LinearSVC(
        C=100,
        dual=False,
        max_iter=20000,
        tol=1e-3,
        class_weight="balanced"
    ))
])

final_svm.fit(X_train, y_train)

final_svm.predict(X_test)



#### ------ Evaluate ----- 
# Random Forest
evaluate_model(rf, X_test, y_test, model_name="Random Forest")

# XGBoost
evaluate_model(xgb, X_test, y_test, model_name="XGBoost")

# SVM 
evaluate_model(final_svm, X_test, y_test, model_name="SVM")