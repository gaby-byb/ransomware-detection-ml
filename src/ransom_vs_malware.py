
import warnings
import joblib
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd

from evaluate import evaluate_model

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report 
from sklearn.svm import SVC
                      
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier


df = pd.read_csv("data/ransom_vs_malware.csv")

#same function as preprocess - modified for this dataset
def preprocess(df):

     # ----- Turn Target to Ransomware vs Non-Ransomware
    # normalize everything to lower case before transforming
    vals = df['Malware_Type'].astype(str).str.lower()

    #match anything containing the keyword ransom
    is_ransomware = vals.str.contains('ransom', na=False)

    #new column - numeric labels 1/0
    df['Ransomware'] = is_ransomware.astype(int)

    #drop original target column
    df = df.drop(columns=['Malware_Type'])

    categorical_cols = [
        'Machine', 'Subsystem', 'Magic',
        'SizeOfOptionalHeader', 'LoaderFlags', 'NumberOfRvaAndSizes'
    ]
    
    #label encoding - replaces each category with an integer code(ok for tree)
    for col in categorical_cols:
        df[col] = df[col].astype('category').cat.codes
    
    # --- Drop Identifiers ---
    meta = df[['SHA256']].copy()
    df = df.drop(columns=['SHA256'])


    # drop highly correlated numeric columns only (needed for SVM)
    corr = df.select_dtypes(include=[np.number]).drop(columns=['Ransomware'], errors='ignore').corr().abs()
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
    drop_cols = [c for c in upper.columns if any(upper[c] > 0.95)]
    df.drop(columns=drop_cols, inplace=True)

    #Mostly needed for linear models - so larger values dont dominate smaller ones
    log_cols = [
        'SizeOfImage', 'SizeOfCode', 'SizeOfInitializedData', 'SizeOfUninitializedData',
        'AddressOfEntryPoint', 'ImageBase', 'BaseOfCode', 'FileAlignment',
        'SectionAlignment', 'SizeOfStackReserve', 'SizeOfHeapReserve',
        'SizeOfHeapCommit', 'SizeOfHeaders', 'PointerToSymbolTable',
        'NumberOfSymbols', 'CheckSum', 'SizeOfOptionalHeader', 'TimeDateStamp'
    ]
    for col in log_cols:
        if col in df.columns:
            df[col] = np.log1p(df[col])

    # Split Features vs Target
    X = df.drop(columns=['Ransomware'])
    y = df['Ransomware']

    return X, y, meta



X, y, meta = preprocess(df)



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

#handling unbalanced data
sm = SMOTE(random_state=42)
X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)


# ---- Random Forest ----


#run the model always using the best parameters(see ipynb)
rf = RandomForestClassifier(n_estimators=761, 
                            max_depth=23,
                            min_samples_leaf=1,
                            min_samples_split=5,
                            random_state=42)

#

rf.fit(X_train_bal, y_train_bal)

y_pred = rf.predict(X_test)



# ---- XGBoost ----



xgb = XGBClassifier(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train),  # balance weight
    random_state=42,
    eval_metric='logloss'  # suppresses warning
)
# Train and evaluate
xgb.fit(X_train_bal, y_train_bal)

# --- XGBoost Threshold Tuning ---
y_proba = xgb.predict_proba(X_test)[:, 1]

for threshold in [0.3, 0.5, 0.7]:
    print(f"\n--- XGBoost Threshold = {threshold} ---")
    y_pred_thresh = (y_proba > threshold).astype(int)
    print(classification_report(y_test, y_pred_thresh, digits=3))



#------ SVM -------

#Only need to scale data for SVM model 
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

svm_model = SVC(kernel='rbf', class_weight='balanced', random_state=42)
svm_model.fit(X_train_scaled, y_train)



print("\n--- Evaluation using AUC ---")

# Random Forest
evaluate_model(rf, X_test, y_test, model_name="Random Forest")

# XGBoost
evaluate_model(xgb, X_test, y_test, model_name="XGBoost")

# SVM 
evaluate_model(svm_model, X_test_scaled, y_test, model_name="SVM")

# --- for the CLI -----

#save best performing model
joblib.dump(xgb, "models/ransom_vs_malware.pkl")

#save train dataset
X_train.to_csv("data/X_ransom_vs_malware.csv", index=False)

#save labels - having to name everything is getting out of control
y_train.to_csv("data/labels_ransom_vs_malware", index=False)
